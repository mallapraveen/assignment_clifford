{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "import boto3\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\",\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    aws_session_token=os.getenv(\"AWS_SESSION_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock, BedrockLLM\n",
    "\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "\n",
    "claude_3 = BedrockLLM(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=model_id,\n",
    ")\n",
    "\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# # Invoke Example\n",
    "# messages = [\n",
    "#     (\"human\", \"{question}\"),\n",
    "# ]\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# chain = prompt | claude_3 | StrOutputParser()\n",
    "\n",
    "# # Chain Invoke\n",
    "# response = chain.invoke({\"question\": \"Hi\"})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loader - LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.llms.langchain import LangChainLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "Settings.llm = LangChainLLM(llm=claude_3)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = LlamaParse(result_type=\"text\")  # \"markdown\" and \"text\" are available\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\n",
    "        \"../data/OJ_L_202401689_EN_TXT.pdf\",\n",
    "    ],\n",
    "    file_extractor=file_extractor,\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# Initialize the node parser with a specified chunk size\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=256)\n",
    "\n",
    "# Parse nodes from the provided documents\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# Create a vector store index from the parsed nodes\n",
    "vector_index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
    "vector_index.storage_context.persist(persist_dir=\"../persist/retreival_eval\")\n",
    "\n",
    "# Generate question-context pairs for evaluation\n",
    "qa_dataset = generate_question_context_pairs(nodes, llm=claude_3)\n",
    "\n",
    "# Save the generated QA dataset to a JSON file\n",
    "qa_dataset.save_json(\"../datasets/retreival_eval.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"../persist/retreival_eval\")\n",
    "\n",
    "# load index\n",
    "vector_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Create a QA dataset for fine-tuning embeddings\n",
    "qa_dataset = EmbeddingQAFinetuneDataset.from_json(\"../datasets/hit_rate_mrr_chunk.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever with semantic search\n",
    "retriever1 = vector_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=vector_index.docstore, similarity_top_k=3\n",
    ")\n",
    "\n",
    "retriever2 = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=3,\n",
    "    num_queries=1,  # set this to 1 to disable query generation\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "api_key = os.environ[\"COHERE_API_KEY\"]\n",
    "cohere_rerank = CohereRerank(api_key=api_key, top_n=3)\n",
    "\n",
    "retriever3 = vector_index.as_retriever(\n",
    "    similarity_top_k=5,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "# Define the evaluation metrics\n",
    "metrics = [\"precision\", \"recall\", \"hit_rate\", \"ap\", \"mrr\", \"ndcg\"]\n",
    "\n",
    "# Create retriever evaluators for each retriever\n",
    "retriever_evaluator1 = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever1\n",
    ")\n",
    "retriever_evaluator2 = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever2\n",
    ")\n",
    "retriever_evaluator3 = RetrieverEvaluator.from_metric_names(\n",
    "    metrics, retriever=retriever3\n",
    ")\n",
    "\n",
    "# Evaluate the dataset using the retriever evaluators\n",
    "eval_results1 = await retriever_evaluator1.aevaluate_dataset(\n",
    "    qa_dataset, show_progress=True, workers=1\n",
    ")\n",
    "eval_results2 = await retriever_evaluator2.aevaluate_dataset(\n",
    "    qa_dataset, show_progress=True, workers=1\n",
    ")\n",
    "eval_results3 = await retriever_evaluator3.aevaluate_dataset(\n",
    "    qa_dataset, show_progress=True, workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def display_results(name, eval_results):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    columns = {\n",
    "        \"retrievers\": [name],\n",
    "        **{k: [full_df[k].mean()] for k in metrics},\n",
    "    }\n",
    "\n",
    "    metric_df = pd.DataFrame(columns)\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df1 = display_results(\"Semantic Search\", eval_results1)\n",
    "metric_df2 = display_results(\"Hybrid Search\", eval_results2)\n",
    "metric_df3 = display_results(\"With Reranker\", eval_results3)\n",
    "\n",
    "results = pd.concat([metric_df1, metric_df2, metric_df3], ignore_index=True)\n",
    "results.to_csv(\"../data/results/retrieval-evaluation.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
