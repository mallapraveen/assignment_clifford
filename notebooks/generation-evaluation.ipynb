{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\",\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    aws_session_token=os.getenv(\"AWS_SESSION_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_id = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "\n",
    "claude_3 = ChatBedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=model_id,\n",
    ")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"BAAI/bge-small-en-v1.5\"  # BAAI/llm-embedder, BAAI/bge-large-en-v1.5, mixedbread-ai/mxbai-embed-large-v1, TextEmbeddingAda2\n",
    "llm = claude_3\n",
    "ensemble = (\"ensemble\", 0.5)  # bm25, semantic, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "eval_dataset = pd.read_csv(\"./datasets/synthetic_dataset_hi_res.csv\")\n",
    "test_questions = eval_dataset[\"user_input\"].values.tolist()\n",
    "eval_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag.rag_pipeline import get_retriever_parent_child, get_qa_chain_rerank\n",
    "\n",
    "retriever, bm25_retriever = get_retriever_parent_child(\n",
    "    file_path=\"./data\", model_name=embedding_model\n",
    ")\n",
    "qa_chain = get_qa_chain_rerank(retriever, bm25_retriever, ensemble[1], llm)\n",
    "value = qa_chain({\"query\": test_questions[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "responses = [qa_chain({\"query\": q}) for q in test_questions]\n",
    "\n",
    "# Extract answers and contexts from responses\n",
    "answers = []\n",
    "contexts = []\n",
    "for r in responses:\n",
    "    answers.append(r[\"result\"])\n",
    "    contexts.append(\n",
    "        [\n",
    "            dict(c)[\"page_content\"]\n",
    "            for c in r[\"source_documents\"]\n",
    "            if \"page_content\" in dict(c)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Create a dictionary for the dataset\n",
    "dataset_dict = {\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "}\n",
    "\n",
    "# Create a Dataset object from the dictionary\n",
    "result_ds = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset[\"answer\"] = result_ds.to_pandas()[\"answer\"]\n",
    "eval_dataset.to_csv(\n",
    "    f\"./datasets/experiments/{llm.model_name}_bge-small-en_{ensemble[0]}.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - LLM Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "dataset = []\n",
    "df = pd.read_csv(\"./data/experiments/GPT4o128k_TextEmbeddingAda2_semantic.csv\")\n",
    "for i in range(len(df)):\n",
    "    dataset.append(\n",
    "        {\n",
    "            \"user_input\": df.loc[0, \"user_input\"],\n",
    "            \"retrieved_contexts\": ast.literal_eval(df.loc[0, \"reference_contexts\"]),\n",
    "            \"response\": df.loc[0, \"answer\"],\n",
    "            \"reference\": df.loc[0, \"reference\"],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmalla01/Desktop/rag-chatbot/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 624/624 [20:39<00:00,  1.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm_context_precision_with_reference': 1.0000, 'context_recall': 1.0000, 'context_entity_recall': 0.0601, 'answer_relevancy': 0.9418, 'faithfulness': 0.9519, 'factual_correctness': 0.4123}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas import RunConfig\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 8},\n",
    ")\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(claude_3)\n",
    "embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "from ragas.metrics import (\n",
    "    LLMContextPrecisionWithReference,  # Generation\n",
    "    LLMContextRecall,\n",
    "    ContextEntityRecall,\n",
    "    ResponseRelevancy,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,  # Generation\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[\n",
    "        LLMContextPrecisionWithReference(),\n",
    "        LLMContextRecall(),\n",
    "        ContextEntityRecall(),\n",
    "        ResponseRelevancy(),\n",
    "        Faithfulness(),\n",
    "        FactualCorrectness(),\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=RunConfig(timeout=600, max_retries=10, max_wait=240, max_workers=4, seed=42),\n",
    "    # callbacks=[cost_cb],\n",
    "    # token_usage_parser=get_token_usage_for_openai,\n",
    "    show_progress=True\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GPT4o128k_bge-small-en_bm25 - {'llm_context_precision_with_reference': 1.0000, 'context_recall': 1.0000, 'context_entity_recall': 0.0793, 'answer_relevancy': 1.0000, 'faithfulness': 0.5048, 'factual_correctness': 0.8000}\n",
    "\n",
    "2. GPT4o128k_bge-small-en_ensemble - {'llm_context_precision_with_reference': 1.0000, 'context_recall': 1.0000, 'context_entity_recall': 0.0625, 'answer_relevancy': 1.0000, 'faithfulness': 0.5000, 'factual_correctness': 0.8000}\n",
    "\n",
    "3. GPT4o128k_bge-small-en_semantic - {'llm_context_precision_with_reference': 1.0000, 'context_recall': 1.0000, 'context_entity_recall': 0.0649, 'answer_relevancy': 0.9418, 'faithfulness': 0.9615, 'factual_correctness': 0.4123}\n",
    "\n",
    "4. GPT4o128k_TextEmbeddingAda2_bm25 - {'llm_context_precision_with_reference': 1.0000, 'context_recall': 1.0000, 'context_entity_recall': 0.0697, 'answer_relevancy': 1.0000, 'faithfulness': 1.0000, 'factual_correctness': 0.7067}\n",
    "\n",
    "5. GPT4o128k_TextEmbeddingAda2_ensemble - {'llm_context_precision_with_reference': 1.0000, 'context_recall': 1.0000, 'context_entity_recall': 0.0649, 'answer_relevancy': 1.0000, 'faithfulness': 1.0000, 'factual_correctness': 0.7087}\n",
    "\n",
    "6. GPT4o128k_TextEmbeddingAda2_semantic - {'llm_context_precision_with_reference': 1.0000, 'context_recall': 1.0000, 'context_entity_recall': 0.0601, 'answer_relevancy': 0.9418, 'faithfulness': 0.9519, 'factual_correctness': 0.4123}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation - NLP Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "class SentenceSimilarity:\n",
    "\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(self.model_name)\n",
    "\n",
    "    def compute_similarity(self, sent1, sent2):\n",
    "        # Convert the sentences into embeddings using the Sentence Transformer\n",
    "        sent_embedding1 = self.model.encode(sent1, convert_to_tensor=True)\n",
    "        sent_embedding2 = self.model.encode(sent2, convert_to_tensor=True)\n",
    "\n",
    "        # Find the similarity between the two embeddings\n",
    "        similarities_sbert = util.pairwise_cos_sim(sent_embedding1, sent_embedding2)\n",
    "        return similarities_sbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setence_similarity(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    similarity_calculator = SentenceSimilarity()\n",
    "\n",
    "    similarities = similarity_calculator.compute_similarity(\n",
    "        data[\"answer\"].tolist(), data[\"reference\"].tolist()\n",
    "    )\n",
    "\n",
    "    return similarities.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from nltk import download as nltk_download, word_tokenize\n",
    "\n",
    "\n",
    "class NLPMetrics:\n",
    "    def __init__(self):\n",
    "        self._rouge = Rouge()\n",
    "\n",
    "    def calculate_rouge(self, response: str, ground_truth: str):\n",
    "        scores = self._rouge.get_scores(response, ground_truth, avg=True)\n",
    "        return scores[\"rouge-l\"][\"p\"], scores[\"rouge-l\"][\"r\"], scores[\"rouge-l\"][\"f\"]\n",
    "\n",
    "    def calculate_bleu(self, response: str, ground_truth: str):\n",
    "        gt_tokens = [word_tokenize(ground_truth)]\n",
    "        res_tokens = word_tokenize(response)\n",
    "        return sentence_bleu(gt_tokens, res_tokens)\n",
    "\n",
    "    def calculate_token_overlap(self, response: str, ground_truth: str):\n",
    "        gt_tokens = set(word_tokenize(ground_truth))\n",
    "        res_tokens = set(word_tokenize(response))\n",
    "        overlap = res_tokens & gt_tokens\n",
    "        precision = len(overlap) / len(res_tokens) if res_tokens else 0\n",
    "        recall = len(overlap) / len(gt_tokens) if gt_tokens else 0\n",
    "        f1_score = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if precision + recall > 0\n",
    "            else 0\n",
    "        )\n",
    "        return precision, recall, f1_score\n",
    "\n",
    "    def calculate_metrics(self, response: str, ground_truth: str):\n",
    "        rouge_p, rouge_r, rouge_f1 = self.calculate_rouge(response, ground_truth)\n",
    "        token_p, token_r, token_f1 = self.calculate_token_overlap(\n",
    "            response, ground_truth\n",
    "        )\n",
    "        bleu = self.calculate_bleu(response, ground_truth)\n",
    "        return {\n",
    "            \"rouge_l_precision\": rouge_p,\n",
    "            \"rouge_l_recall\": rouge_r,\n",
    "            \"rouge_l_f1\": rouge_f1,\n",
    "            \"token_overlap_precision\": token_p,\n",
    "            \"token_overlap_recall\": token_r,\n",
    "            \"token_overlap_f1\": token_f1,\n",
    "            \"bleu_score\": bleu,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_metrics(file_path):\n",
    "    # Initialize the metrics calculator\n",
    "    metrics = NLPMetrics()\n",
    "\n",
    "    # Read the dataset\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Calculate metrics for each pair of response and ground truth\n",
    "    deterministic_scores = [\n",
    "        metrics.calculate_metrics(answer, ground_truth)\n",
    "        for answer, ground_truth in zip(data[\"answer\"], data[\"reference\"])\n",
    "    ]\n",
    "    return deterministic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_metrics(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    similarities = setence_similarity(file_path)\n",
    "    deterministic_scores = deterministic_metrics(file_path)\n",
    "\n",
    "    df_deterministic = pd.DataFrame(deterministic_scores)\n",
    "    df_semantic = pd.DataFrame(\n",
    "        {\n",
    "            \"semantic_similarity_score\": similarities,\n",
    "        }\n",
    "    )\n",
    "    df = pd.concat([df_deterministic, df_semantic], axis=1)\n",
    "\n",
    "    return pd.concat([data, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_bm25 = nlp_metrics(\"../data/experiments/GPT4o128k_bge-small-en_bm25.csv\")\n",
    "bge_semantic = nlp_metrics(\"../data/experiments/GPT4o128k_bge-small-en_semantic.csv\")\n",
    "bge_ensemble = nlp_metrics(\"../data/experiments/GPT4o128k_bge-small-en_ensemble.csv\")\n",
    "ada_bm25 = nlp_metrics(\"../data/experiments/GPT4o128k_TextEmbeddingAda2_bm25.csv\")\n",
    "ada_semantic = nlp_metrics(\"../data/experiments/GPT4o128k_TextEmbeddingAda2_semantic.csv\")\n",
    "ada_ensemble = nlp_metrics(\"../data/experiments/GPT4o128k_TextEmbeddingAda2_ensemble.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['user_input', 'reference_contexts', 'reference', 'synthesizer_name','answer']\n",
    "df = pd.concat(\n",
    "    [\n",
    "        bge_bm25.drop(columns=cols).mean().rename(\"GPT4o128k_bge-small-en_bm25\"),\n",
    "        bge_semantic.drop(columns=cols).mean().rename(\"GPT4o128k_bge-small-en_semantic\"),\n",
    "        bge_ensemble.drop(columns=cols).mean().rename(\"GPT4o128k_bge-small-en_ensemble\"),\n",
    "        ada_bm25.drop(columns=cols).mean().rename(\"GPT4o128k_TextEmbeddingAda2_bm25\"),\n",
    "        ada_semantic.drop(columns=cols).mean().rename(\"GPT4o128k_TextEmbeddingAda2_semantic\"),\n",
    "        ada_ensemble.drop(columns=cols).mean().rename(\"GPT4o128k_TextEmbeddingAda2_ensemble\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ").T\n",
    "df[\"Pipeline\"] = [\n",
    "    \"GPT4o128k_bge-small-en_bm25\",\n",
    "    \"GPT4o128k_bge-small-en_semantic\",\n",
    "    \"GPT4o128k_bge-small-en_ensemble\",\n",
    "    \"GPT4o128k_TextEmbeddingAda2_bm25\",\n",
    "    \"GPT4o128k_TextEmbeddingAda2_semantic\",\n",
    "    \"GPT4o128k_TextEmbeddingAda2_ensemble\",\n",
    "]\n",
    "\n",
    "df.to_csv(\"../data/results/generation-evaluation-NLP-based.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
